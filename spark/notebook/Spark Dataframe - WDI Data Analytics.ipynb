{
  "metadata": {
    "name": "Spark Dataframe - WDI Data Analytics",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# SparkSession\n\nSparkSession is automatically created when you start up a Notebook (e.g. Zeppelin, Databricks)\n\n\u003cimg src\u003d\"https://i.imgur.com/5Ai45fb.jpg\" width\u003d500px\u003e"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n//Scala SparkSession\nspark"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n#PySpark SparkSession\nspark"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": " \n\n# Show DataFrame\n\n`df.show()` is the Spark native API that displays data but it\u0027s not pretty. \n\n`z.show(df)` is a Zeppelin build-in feature that allows you to show a df result in a pretty way"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\n#List all hive tables in a df\ntables_df \u003d spark.sql(\"show tables\")"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\ntables_df.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nz.show(tables_df)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": " \n\n# Spark SQL vs Dataframe\n\n`%sql` is the Spark SQL interpreter\n\n`%spark.pyspark` is the PySpark interpreter\n\n`%spark` is the Spark Scala interpreter"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\n\nselect count(1) from wdi_csv_parquet"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\n#Read Hive data to a df (this is lazy)\nwdi_df \u003d spark.sql(\"SELECT * from wdi_csv_parquet\")\n#Persist df in memory for fast futuer access\nwdi_df \u003d wdi_df.cache()\nwdi_df.printSchema()\n\n#Spark action is eager\nz.show(wdi_df.count())"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": " \n\n# Show Historical GDP for Canada\n\n- Re-write the hive query (left cell) using PySpark df"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\nSELECT year, IndicatorValue as GDP\nFROM wdi_csv_parquet\nWHERE indicatorCode \u003d \u0027NY.GDP.MKTP.KD.ZG\u0027 and countryName \u003d \u0027Canada\u0027\nORDER BY year\n"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfrom pyspark.sql.functions import col\nfrom pyspark.sql.functions import desc\n\nwdi_canada_df \u003d wdi_df\\\n    .where(col(\u0027countryname\u0027) \u003d\u003d \u0027Canada\u0027)\\\n    .where(col(\u0027indicatorcode\u0027) \u003d\u003d \u0027NY.GDP.MKTP.KD.ZG\u0027)\\\n    .sort(desc(\u0027year\u0027))\n\n\n#use z.show to display df result and draw a bar chart\nz.show(wdi_canada_df.select(\"year\", \"IndicatorValue\"))"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": " \n\n# Show GDP for Each County and Sort By Year\n\n- Re-write the hive query (left cell) using PySpark df  \n    - hint: you can create multiple DFs "
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\nSELECT countryname,\n       year,\n       indicatorcode,\n       indicatorvalue\nFROM wdi_csv_parquet\nWHERE indicatorcode \u003d \u0027NY.GDP.MKTP.KD.ZG\u0027\nDISTRIBUTE BY countryname\nSORT BY countryname, year\n"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\n#Your solution goes here (please remove this commnet from you copy)\nfrom pyspark.sql.functions import col\n\nwdi_gdp_df \u003d wdi_df\\\n    .where(col(\u0027indicatorcode\u0027) \u003d\u003d \u0027NY.GDP.MKTP.KD.ZG\u0027)\\\n    .sort([\u0027countryname\u0027, \u0027year\u0027], ascending\u003dTrue)\n\n#use z.show to display df result and draw a bar chart\nz.show(wdi_gdp_df.select(\"countryname\", \"year\", \"indicatorcode\", \"IndicatorValue\"))"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Find the highest GDP for each country\n\n- Re-write the hive query (left cell) using PySpark df\n"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\n\nSELECT wdi_csv_parquet.indicatorvalue AS value, \n       wdi_csv_parquet.year           AS year, \n       wdi_csv_parquet.countryname    AS country \nFROM   (SELECT Max(indicatorvalue) AS ind, \n               countryname \n        FROM   wdi_csv_parquet \n        WHERE  indicatorcode \u003d \u0027NY.GDP.MKTP.KD.ZG\u0027 \n               AND indicatorvalue \u003c\u003e 0 \n        GROUP  BY countryname) t1 \n       INNER JOIN wdi_csv_parquet \n               ON t1.ind \u003d wdi_csv_parquet.indicatorvalue \n                  AND t1.countryname \u003d wdi_csv_parquet.countryname\n"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfrom pyspark.sql.functions import max\nfrom pyspark.sql.functions import count\n\nwdi_max_gdp_df \u003d wdi_df\\\n    .where(col(\u0027indicatorcode\u0027) \u003d\u003d \u0027NY.GDP.MKTP.KD.ZG\u0027)\\\n    .where(col(\u0027indicatorvalue\u0027) !\u003d 0)\\\n    .groupby(col(\u0027countryname\u0027))\\\n    .agg(max(\u0027indicatorvalue\u0027).alias(\u0027indicatorvalue\u0027))\\\n    .join(wdi_df, [\u0027countryname\u0027, \u0027indicatorvalue\u0027])\n\n    \nz.show(wdi_max_gdp_df.select(\"indicatorvalue\", \"year\", \"countryname\"))"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n"
    }
  ]
}